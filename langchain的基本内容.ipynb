{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from utils.env_loader import get_env\n",
    "api_key = get_env(\"ZHIPUAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "# 创建CharModels\n",
    "# ChatModels输入是一个ChatMEssage列表\n",
    "# ChatModels输出是一个单独的ChatMessage\n",
    "# ChatMessage的必要组件：content和role（由ChatMessage来定义的实体对象）\n",
    "# Langchain提供HumanMessage，AIMessage，SystemMessage，FunctionMessage，可以通过ChatMessage自定义角色\n",
    "\n",
    "# 这是使用ZhipuAI的标准模板，\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.95,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37643/323288062.py:6: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm.predict_messages(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='制造多彩袜子的公司可以考虑以下一些有创意的名字，这些名字旨在体现出产品丰富多彩的特点，同时也要易于记忆和传播：\\n\\n1. 彩纬编织\\n2. 缤纷足语\\n3. 彩绘袜界\\n4. 色彩踏步\\n5. 梦幻袜彩\\n6. 色彩漫步\\n7. 彩虹桥袜业\\n8. 色彩足迹\\n9. 风彩袜艺\\n10. 彩韵袜品\\n\\n选择名字时，还应该考虑以下因素：\\n- 名字是否已被注册或商标保护，避免侵权。\\n- 名字是否易于在国内外市场发音和传播。\\n- 名字是否能够体现出公司的文化理念和产品特色。\\n\\n希望这些建议能够帮助您找到一个既响亮又吸引人的公司名字！', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 176, 'prompt_tokens': 14, 'total_tokens': 190, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-74b90b0a-cf2d-4397-a836-8df9d73796bd-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict_message方法对消息列表处理，返回的也是消息类型\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"制造多彩袜子的公司有什么好名字？\"\n",
    "message = [HumanMessage(content=text)]\n",
    "\n",
    "llm.predict_messages(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'制造玩具的公司有什么好名字？'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# 这里可以看到使用模板函数时指定的参数用{}标出，参数为其中的内容\n",
    "prompt = PromptTemplate.from_template(\"制造{product}的公司有什么好名字？\")\n",
    "prompt.format(product=\"玩具\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import(\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "# 显式定义模板\n",
    "system_template = \"你是一个强力的翻译助手，可以将{input_language}，翻译为{output_language}.\"\n",
    "sys_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([sys_message_prompt, human_message_prompt])\n",
    "# 这里定义模板参数\n",
    "messages = chat_prompt.format_messages(input_language=\"英语\",output_language=\"中文\",text=\"i Love programming.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input_language', 'output_language', 'text'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], input_types={}, partial_variables={}, template='你是一个强力的翻译助手，可以将{input_language}，翻译为{output_language}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='{text}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='你是一个强力的翻译助手，可以将英语，翻译为中文.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='i Love programming.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"我喜欢编程。 \\n\\nIf you need any translation help, feel free to ask. If you have any English text you'd like to translate into Chinese, just provide it and I'll do my best to translate it for you.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 25, 'total_tokens': 72, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1a875ea3-9040-4d16-93ab-8199c55867c0-0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        return text.strip().split(\"，\")\n",
    "\n",
    "CommaSeparatedListOutputParser().parse(\"hi，bye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMChian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将多种组件组合到一起形成一个链式组件\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"我喜欢编程。 \\n\\nIf you need any translation help, feel free to ask me. If you have any specific sentences or texts you'd like to translate, just let me know!\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在运行时补全所有的参数\n",
    "chain.run(input_language=\"英文\",output_language=\"中文\", text=\"i love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL Langchain Expression Language\n",
    "构建组合链式操作chains的声明语言\n",
    "\n",
    "以下是这个过程的说明示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# 这一过程可以参考LLMchain\n",
    "# 也即这个链式可以表示为 prompt | model | output_parser\n",
    "\n",
    "# 可以表示为\n",
    "chain = LLMChain(\n",
    "    # PromptTemplate 类型\n",
    "    # ChatModel类型\n",
    "    # OutputParser类型\n",
    ")\n",
    "# 注意prompt的参数可以在调用chain前就指明也可以调用时再指明\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "# 指明参数\n",
    "prompt_valued = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "\n",
    "# 这里当langchian没有默认支持时\n",
    "# 需要查看各模型SDK的支持方案\n",
    "model = ChatOpenAI(\n",
    "    # 模型参数\n",
    ")\n",
    "# 此时可以接收指定参数后的prompt\n",
    "message = model.invoke(prompt_valued)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "# 此时可以直接解析message\n",
    "output = output_parser(message)\n",
    "\n",
    "# 也可以表示为\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个RAG简单过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ben worked at Shanghai.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检索向量数据库\n",
    "# 生成prompt\n",
    "# 将prompt输入到模型\n",
    "# 处理模型返回数据\n",
    "from utils.env_loader import get_env\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# 这里若需要多种不同的模板则可导入更多PromptTemplate类型\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from embedding.zhipu_embedding import ZhipuAIEmbeddings\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"ben worked at shanghai\", \"kety likes playing ball\"],\n",
    "    embedding=ZhipuAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "api_key = get_env(\"ZHIPUAI_API_KEY\")\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "model = ChatOpenAI(\n",
    "    temperature=0.95,\n",
    "    model=\"glm-4\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "# 这里将retriever和RunnablePassthrough拼在一起\n",
    "# 这里retriever会根据RunnablePassthrough传递的prompt['question']内容检索\n",
    "# 然后生成一个检索的内容\n",
    "# 最终setup_and_retriever返回\n",
    "# {\"context\":, \"question\":}\n",
    "setup_and_retriever = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "chain = setup_and_retriever | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did ben work?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ben worked at shanghai'),\n",
       " Document(page_content='kety likes playing ball')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"where did ben work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['DocArrayInMemorySearch'], vectorstore=<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x7fb0da8f69c0>),\n",
       "  question: RunnablePassthrough()\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='ben worked at shanghai'),\n",
       "  Document(page_content='kety likes playing ball')],\n",
       " 'question': 'where is ben work'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 说明RunnableParallel在做什么\n",
    "# 他接收参数\n",
    "# 将retriever的内容放在键为\"context\"下\n",
    "# 将传入内容通过Runnablepassthrough()放在键question下\n",
    "\n",
    "\"\"\"\n",
    "下面这个组件放在链里会直接将参数传给prompt\n",
    "类似于\n",
    "setup_and_retriever | prompt\n",
    "变为了\n",
    "prompt({'context':retriever, 'question':'where is ben work'})\n",
    "\"\"\" \n",
    "\n",
    "setup_and_retriever.invoke(\"where is ben work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Answer the question based only on the following context:\\ntest\\n\\nQuestion: test\\n\\n')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"context\": \"test\", \"question\": \"test\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个流式传输的例子\n",
    "\n",
    "流式传输是为了更好的与用户交互，如果不使用流式传输，用户将等待很长时间直接看到一长串内容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好|！|我是一个|人工智能|助手|，|基于|大型|语言|模型|开发|而成|，|我的|任务是|针对|用户|的问题|和要求|提供|适当的|答复|和支持|。|我可以|处理|多种|语言|任务|，|包括|文本|生成|、|翻译|、|信息|检索|等|。|我|由|清华大学| K|EG| 实|验|室|与|智|谱|AI|共同|训练|，|我的|目标|是通过|与|人类的|互动|，|帮助|人们|获取|信息|、|解决问题|，|让|生活|变得更加|便捷|。|我|遵循|中国|政府的|立场|，|坚守|社会主义|价值观|。|在与|你|互动|的过程中|，|我会|不断|学习和|进步|，|以期|提供|更|优质|的服务|。||"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "\n",
    "async for chunk in model.astream(\"你好告诉我一些关于你的事情\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的|，|这里|有一个|关于|鹦|鹉|的|笑话|：\n",
      "\n",
      "有一天|，|一个人|去|宠物|店|买|了一只|鹦|鹉|。|他|问|鹦|鹉|：“|你会|说话|吗|？”|鹦|鹉|回答|：“|当然|会|，|我|可是|只|聪明的|鹦|鹉|。”\n",
      "\n",
      "回家|后|，|主人|发现|这只|鹦|鹉|确实|很|聪明|，|它能|模仿|各种|声音|，|甚至|能|学说|一些|简单的|词语|。|有一天|，|主|人在|厨房|做饭|，|鹦|鹉|在|客厅|里|玩|。|突然|，|主人|听到|鹦|鹉|在|尖叫|：“|救命|啊|！|救命|啊|！|”\n",
      "\n",
      "主人|急忙|跑|出|厨房|，|发现|鹦|鹉|正|吊|在|窗帘|上|，|脚|被|窗帘|绳|缠|住了|。|主人|赶紧|把|鹦|鹉|解|救|下来|，|问|它|：“|你|刚才|为什么|喊|救命|？|”\n",
      "\n",
      "鹦|鹉|答|道|：“|我只是|想|试试|，|看|你会|不会|真的|来|救|我|。”\n",
      "\n",
      "主人|哭|笑|不得|，|心想|：|原来|这只|鹦|鹉|不仅|聪明|，|还|这么|调皮|。||"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"告诉我一个关于{topic}的笑话\")\n",
    "\n",
    "# 从输出中可以发现，这个解析器会将模型的输出进行换行解析\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\" : \"鹦鹉\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\n",
    "         \".......\"\n",
    "         ),\n",
    "        (\"human\",\"........\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='.......')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='........'))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunnablePassthrough:传递参数\n",
    "\n",
    "# ChatOpenAI().bind():绑定运行时参数\n",
    "如何时停止，对于模型特定的运行脚本\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
